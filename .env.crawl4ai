# MiniMax M2 Configuration (OpenAI-compatible API)
LLM_API_BASE=https://api.minimax.io/v1
LLM_API_KEY=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJHcm91cE5hbWUiOiJQYXJ0aCBTaGV0aCIsIlVzZXJOYW1lIjoiUGFydGggU2hldGgiLCJBY2NvdW50IjoiIiwiU3ViamVjdElEIjoiMTk4NDI2MzUzNDY1NzE1MTIyMiIsIlBob25lIjoiIiwiR3JvdXBJRCI6IjE5ODQyNjM1MzQ2NTI5NTI4MjIiLCJQYWdlTmFtZSI6IiIsIk1haWwiOiJwYXJ0aHNoZXRoMzI2QGdtYWlsLmNvbSIsIkNyZWF0ZVRpbWUiOiIyMDI1LTExLTAxIDA0OjMzOjU2IiwiVG9rZW5UeXBlIjoxLCJpc3MiOiJtaW5pbWF4In0.qi46w5DVb7B78g2ejaxGck7RDmDUqVyOF2VRaFVW5fpNAhjTn7eMRLUhAaum12G07jXGyVyK7CWoFW8YXtQpTQpJdH4sQ--77k1JL-PYgwS5NU6k5HvoSKXTm647zEIwzRQqBbLYEzlwVR9sO3ptroES-hpGdLEf4BIT1lhFcX5IkB5wCvQcNzEWrYNpOckTudNigHN_XcB0cfUq8eNmLPTR7JdXYKI2snmkxCt0pJPr-PhT2DAxKJUChmj-S0AEWVSs_ClCNSLGJimF9uKttPtXWJGUYaPCKGPmKrxXsWpFheCoFUgi4XXYvYgcC8tAqSPjCVmoqlV8wdgwXzjTrQ
MODEL_NAME=MiniMax-M2
# EmbeddingMonster Configuration (Local Services)
EMBEDDING_GTE_PORT=30001
EMBEDDING_CODERANK_PORT=30002

# Postgres/pgvector Configuration
POSTGRES_CONNECTION_STRING=postgresql://postgres:code-context-secure-password@postgres:5432/claude_context

# Qdrant Configuration
QDRANT_URL=http://qdrant:6333
QDRANT_API_KEY=

# Chunking Configuration (Research-optimized for GTE-768d + CodeRank)
# Documentation: 1600 chars ≈ 400 tokens (optimal for technical content)
# Code: Handled separately via markdown extraction (512 chars ≈ 128 tokens)
CHUNK_SIZE=1600          # Optimal: 400 tokens for GTE embeddings
CHUNK_OVERLAP=200        # 12.5% overlap (research-backed sweet spot)
ENABLE_TREE_SITTER=true  # AST-based code boundary detection

# Scope Configuration
DEFAULT_SCOPE=local


ENABLE_RERANKING=true          # Uses your port 30003 reranker
ENABLE_HYBRID_SEARCH=true       # Uses SPLADE at port 30004
ENABLE_SYMBOL_EXTRACTION=true   # Already enabled by default

# Parallel Embedding Configuration
ENABLE_PARALLEL_EMBEDDING=true  # Process GTE and CodeRank embeddings concurrently
MAX_EMBEDDING_CONCURRENCY=2     # Max models to run in parallel (2 = GTE + CodeRank)
EMBEDDING_METRICS_ENABLED=true  # Enable detailed timing metrics for embedding performance

# Streaming Pipeline Configuration
ENABLE_STREAMING_PIPELINE=false  # Disabled - using hybrid batch-stream instead
STREAMING_CHUNK_WORKERS=4        # Concurrent chunking workers
STREAMING_EMBED_WORKERS=2        # Concurrent embedding workers
STREAMING_STORE_WORKERS=2        # Concurrent storage workers

# Hybrid Batch-Stream Configuration (ACTIVE)
PROCESSING_MODE=sequential       # Options: sequential, micro_batch, hybrid, streaming
                                 # NOTE: hybrid mode doesn't support recursive crawling!
HYBRID_CRAWL_BATCH=50           # Crawl N pages in parallel async
HYBRID_PROCESS_BATCH=10         # Process N pages at a time (GPU-safe)
HYBRID_MAX_MEMORY_PAGES=100     # Max pages in memory at once

# GPU Memory Protection (CRITICAL - Prevents OOM)
GTE_MAX_BATCH_SIZE=16           # Reduced from 32 → safe for GPU
GTE_MAX_CONCURRENT_REQUESTS=8   # Reduced from 32 → safe for GPU
CODERANK_MAX_BATCH_SIZE=16      # Match GTE settings
CODERANK_MAX_CONCURRENT_REQUESTS=8

# Recursive Crawling Configuration (Parallel Batching)
CRAWL_BATCH_SIZE=50             # URLs to crawl in each parallel batch
CRAWL_MAX_CONCURRENT=10         # Max parallel page fetches per batch
MEMORY_THRESHOLD_PERCENT=80     # Auto-throttle when memory > 80%
CRAWL_PAGE_TIMEOUT=10000        # Page load timeout in milliseconds (10s - skip slow pages)
CRAWL_WAIT_STRATEGY=domcontentloaded  # Options: domcontentloaded, networkidle, load

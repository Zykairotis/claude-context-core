/**
 * Incremental sync orchestration for efficient codebase updates
 */

import * as fs from 'fs';
import * as path from 'path';
import { Pool } from 'pg';
import { Context } from '../context';
import { 
    detectChanges, 
    detectRenames, 
    ChangeSummary, 
    FileChange 
} from './change-detector';
import {
    recordFileMetadata,
    updateFileMetadata,
    removeFileMetadata,
    updateFilePath,
    clearDatasetMetadata
} from './file-metadata';

export interface SyncStats {
    filesScanned: number;
    filesCreated: number;
    filesModified: number;
    filesDeleted: number;
    filesRenamed: number;
    filesUnchanged: number;
    chunksAdded: number;
    chunksRemoved: number;
    chunksUnchanged: number;
    durationMs: number;
    scanDurationMs: number;
    syncDurationMs: number;
}

export interface SyncOptions {
    force?: boolean;           // Treat all files as changed
    detectRenames?: boolean;    // Enable rename detection (default: true)
    progressCallback?: (progress: SyncProgress) => void;
}

export interface SyncProgress {
    phase: 'scanning' | 'deleting' | 'updating' | 'creating' | 'renaming' | 'complete';
    current: number;
    total: number;
    percentage: number;
    file?: string;
    detail?: string;
}

/**
 * Delete all chunks for a file from vector database
 */
async function deleteFileChunks(
    context: Context,
    filePath: string,
    codebasePath: string,
    project: string,
    dataset: string,
    collectionName: string
): Promise<number> {
    try {
        const relativePath = path.relative(codebasePath, filePath);
        const vectorDb = context.getVectorDatabase();
        const scopeManager = context.getScopeManager();
        
        // Get deterministic IDs for filtering
        const projectId = scopeManager.getProjectId(project);
        const datasetId = scopeManager.getDatasetId(dataset);
        
        // Use the new deleteByFilePath method
        const deleted = await vectorDb.deleteByFilePath(
            collectionName,
            relativePath,
            projectId,
            datasetId
        );
        
        if (deleted && deleted > 0) {
            console.log(`[IncrementalSync] üóëÔ∏è  Deleted ${deleted} chunks for ${relativePath}`);
            return deleted;
        } else {
            // Qdrant doesn't return count, assume success if no error
            console.log(`[IncrementalSync] üóëÔ∏è  Requested deletion for chunks in ${relativePath}`);
            return 0; // Unknown count, but operation succeeded
        }
    } catch (error) {
        console.error(`[IncrementalSync] ‚ùå Failed to delete chunks for ${filePath}:`, error);
        // Don't throw - continue with sync even if deletion fails
        return 0;
    }
}

/**
 * Index a single file and return chunks created
 */
async function indexSingleFile(
    context: Context,
    filePath: string,
    codebasePath: string,
    project: string,
    dataset: string,
    metadata?: any
): Promise<{ chunks: any[]; count: number }> {
    try {
        // Read file content
        const content = await fs.promises.readFile(filePath, 'utf-8');
        const relativePath = path.relative(codebasePath, filePath);
        
        // Use Context's existing chunking logic
        const fileExtension = path.extname(filePath);
        const chunks: any[] = [];
        
        // Parse and chunk the file (simplified version)
        // In reality, this should use Context's AST-aware splitter
        const lines = content.split('\n');
        const chunkSize = 30; // Lines per chunk
        
        for (let i = 0; i < lines.length; i += chunkSize - 5) { // 5 line overlap
            const chunkLines = lines.slice(i, i + chunkSize);
            const chunkContent = chunkLines.join('\n');
            
            if (chunkContent.trim()) {
                chunks.push({
                    content: chunkContent,
                    metadata: {
                        file_path: filePath,
                        relative_path: relativePath,
                        file_extension: fileExtension,
                        start_line: i + 1,
                        end_line: Math.min(i + chunkSize, lines.length),
                        project,
                        dataset,
                        codebasePath,  // CRITICAL: needed for collection name generation
                        ...metadata
                    }
                });
            }
        }
        
        // Generate embeddings and store chunks
        if (chunks.length > 0) {
            // This should use Context's embedding and storage logic
            await context.storeCodeChunks(chunks, project, dataset, {
                repo: metadata?.repo,
                branch: metadata?.branch,
                sha: metadata?.sha
            });
        }
        
        return { chunks, count: chunks.length };
    } catch (error) {
        console.error(`[IncrementalSync] Failed to index ${filePath}:`, error);
        return { chunks: [], count: 0 };
    }
}

/**
 * Main incremental sync function
 */
export async function incrementalSync(
    context: Context,
    pool: Pool,
    codebasePath: string,
    project: string,
    projectId: string,
    dataset: string,
    datasetId: string,
    options: SyncOptions = {}
): Promise<SyncStats> {
    const startTime = Date.now();
    const { force = false, detectRenames: enableRenames = true, progressCallback } = options;
    
    console.log(`[IncrementalSync] üöÄ Starting incremental sync for ${codebasePath}`);
    console.log(`[IncrementalSync] üì¶ Project: ${project}, Dataset: ${dataset}`);
    
    // Step 0: Ensure collection exists (using Context's naming convention)
    let collectionName: string;
    try {
        collectionName = context.getCollectionName(codebasePath);
        const vectorDb = context.getVectorDatabase();
        const exists = await vectorDb.hasCollection(collectionName);
        
        if (!exists) {
            console.log(`[IncrementalSync] üì¶ Creating collection: ${collectionName}`);
            // Always create hybrid collection (default mode)
            await vectorDb.createHybridCollection(collectionName, 768, `Hybrid collection for ${codebasePath}`);
            console.log(`[IncrementalSync] ‚úÖ Collection created: ${collectionName}`);
        } else {
            console.log(`[IncrementalSync] ‚úì Collection exists: ${collectionName}`);
        }
    } catch (collectionError) {
        console.error(`[IncrementalSync] ‚ö†Ô∏è  Collection setup warning:`, collectionError);
        // Fallback to getting collection name even if creation failed
        collectionName = context.getCollectionName(codebasePath);
        // Continue anyway - context.storeCodeChunks will handle it
    }
    
    // Step 1: Detect changes
    progressCallback?.({
        phase: 'scanning',
        current: 0,
        total: 100,
        percentage: 0,
        detail: 'Scanning for changes...'
    });
    
    let changes: ChangeSummary;
    
    if (force) {
        console.log(`[IncrementalSync] ‚ö†Ô∏è  Force mode: treating all files as changed`);
        // Clear all metadata to force full reindex
        const clearedCount = await clearDatasetMetadata(pool, projectId, datasetId);
        console.log(`[IncrementalSync] üóëÔ∏è  Cleared ${clearedCount} file metadata records`);
        
        // Now detect changes (all files will appear as "created")
        changes = await detectChanges(codebasePath, projectId, datasetId, pool);
    } else {
        changes = await detectChanges(codebasePath, projectId, datasetId, pool);
    }
    
    const scanDurationMs = changes.stats.scanDurationMs;
    
    // Step 2: Detect renames if enabled
    let renames: Array<{ old: string; new: string; hash: string }> = [];
    let actualCreated = changes.created;
    let actualDeleted = changes.deleted;
    
    if (enableRenames && !force) {
        const renameResult = detectRenames(changes);
        renames = renameResult.renames;
        actualCreated = renameResult.actualCreated;
        actualDeleted = renameResult.actualDeleted;
    }
    
    // Calculate total operations
    const totalOperations = 
        actualDeleted.length + 
        changes.modified.length + 
        actualCreated.length + 
        renames.length;
    
    let completed = 0;
    const stats: SyncStats = {
        filesScanned: changes.stats.totalFiles,
        filesCreated: actualCreated.length,
        filesModified: changes.modified.length,
        filesDeleted: actualDeleted.length,
        filesRenamed: renames.length,
        filesUnchanged: changes.unchanged.length,
        chunksAdded: 0,
        chunksRemoved: 0,
        chunksUnchanged: 0,
        durationMs: 0,
        scanDurationMs,
        syncDurationMs: 0
    };
    
    const syncStartTime = Date.now();
    
    // Step 3: Process deletions first
    for (const file of actualDeleted) {
        progressCallback?.({
            phase: 'deleting',
            current: completed,
            total: totalOperations,
            percentage: Math.round((completed / totalOperations) * 100),
            file: file.relativePath,
            detail: `Deleting chunks for ${file.relativePath}`
        });
        
        const removed = await deleteFileChunks(context, file.path, codebasePath, project, dataset, collectionName);
        await removeFileMetadata(pool, projectId, datasetId, file.path);
        
        stats.chunksRemoved += removed;
        completed++;
        
        if (removed > 0) {
            console.log(`[IncrementalSync] üóëÔ∏è  Deleted ${removed} chunks for ${file.relativePath}`);
        }
    }
    
    // Step 4: Process modifications
    for (const file of changes.modified) {
        progressCallback?.({
            phase: 'updating',
            current: completed,
            total: totalOperations,
            percentage: Math.round((completed / totalOperations) * 100),
            file: file.relativePath,
            detail: `Updating ${file.relativePath}`
        });
        
        // Delete old chunks
        const removed = await deleteFileChunks(context, file.path, codebasePath, project, dataset, collectionName);
        stats.chunksRemoved += removed;
        
        // Index updated file
        const result = await indexSingleFile(
            context,
            file.path,
            codebasePath,
            project,
            dataset,
            {
                language: file.language,
                relativePath: file.relativePath
            }
        );
        
        stats.chunksAdded += result.count;
        
        // Update metadata
        await updateFileMetadata(pool, {
            projectId,
            datasetId,
            filePath: file.path,
            relativePath: file.relativePath,
            sha256Hash: file.newHash!,
            fileSize: file.size!,
            chunkCount: result.count,
            language: file.language
        });
        
        completed++;
        console.log(`[IncrementalSync] ‚úèÔ∏è  Updated ${file.relativePath}: -${removed}/+${result.count} chunks`);
    }
    
    // Step 5: Process renames
    for (const rename of renames) {
        progressCallback?.({
            phase: 'renaming',
            current: completed,
            total: totalOperations,
            percentage: Math.round((completed / totalOperations) * 100),
            detail: `Renaming ${path.basename(rename.old)} ‚Üí ${path.basename(rename.new)}`
        });
        
        // Update file path in metadata (no need to reindex, content is same)
        const newRelativePath = path.relative(codebasePath, rename.new);
        await updateFilePath(
            pool, 
            projectId, 
            datasetId, 
            rename.old, 
            rename.new, 
            newRelativePath
        );
        
        // Update file_path in vector database chunks
        // This would require a custom update operation in Qdrant
        // For now, we'll note this as a limitation
        console.log(`[IncrementalSync] üîÑ Renamed: ${rename.old} ‚Üí ${rename.new}`);
        
        completed++;
    }
    
    // Step 6: Process new files
    for (const file of actualCreated) {
        progressCallback?.({
            phase: 'creating',
            current: completed,
            total: totalOperations,
            percentage: Math.round((completed / totalOperations) * 100),
            file: file.relativePath,
            detail: `Indexing new file ${file.relativePath}`
        });
        
        // Index new file
        const result = await indexSingleFile(
            context,
            file.path,
            codebasePath,
            project,
            dataset,
            {
                language: file.language,
                relativePath: file.relativePath
            }
        );
        
        stats.chunksAdded += result.count;
        
        // Record metadata
        await recordFileMetadata(pool, {
            projectId,
            datasetId,
            filePath: file.path,
            relativePath: file.relativePath,
            sha256Hash: file.newHash!,
            fileSize: file.size!,
            chunkCount: result.count,
            language: file.language
        });
        
        completed++;
        console.log(`[IncrementalSync] ‚ûï Created ${file.relativePath}: ${result.count} chunks`);
    }
    
    // Calculate unchanged chunks (approximate)
    // This would require querying the database for total chunks
    const unchangedFiles = changes.unchanged;
    let unchangedChunks = 0;
    if (unchangedFiles.length > 0) {
        // Estimate ~20 chunks per file (this should query the database)
        unchangedChunks = unchangedFiles.length * 20;
    }
    stats.chunksUnchanged = unchangedChunks;
    
    // Final timing
    stats.syncDurationMs = Date.now() - syncStartTime;
    stats.durationMs = Date.now() - startTime;
    
    progressCallback?.({
        phase: 'complete',
        current: totalOperations,
        total: totalOperations,
        percentage: 100,
        detail: `Sync completed in ${stats.durationMs}ms`
    });
    
    console.log(`[IncrementalSync] ‚úÖ Sync complete!`);
    console.log(`[IncrementalSync] üìä Stats:`, {
        files: `${stats.filesCreated} created, ${stats.filesModified} modified, ${stats.filesDeleted} deleted, ${stats.filesRenamed} renamed, ${stats.filesUnchanged} unchanged`,
        chunks: `${stats.chunksAdded} added, ${stats.chunksRemoved} removed, ${stats.chunksUnchanged} unchanged`,
        timing: `${stats.scanDurationMs}ms scan + ${stats.syncDurationMs}ms sync = ${stats.durationMs}ms total`
    });
    
    return stats;
}

/**
 * Force full reindex (clear metadata and reindex everything)
 */
export async function forceFullReindex(
    context: Context,
    pool: Pool,
    codebasePath: string,
    project: string,
    projectId: string,
    dataset: string,
    datasetId: string,
    progressCallback?: (progress: SyncProgress) => void
): Promise<SyncStats> {
    console.log(`[IncrementalSync] üîÑ Force full reindex requested`);
    
    return incrementalSync(
        context,
        pool,
        codebasePath,
        project,
        projectId,
        dataset,
        datasetId,
        {
            force: true,
            detectRenames: false,
            progressCallback
        }
    );
}

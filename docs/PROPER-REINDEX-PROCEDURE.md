# Proper Re-indexing Procedure for Dataset Isolation

## ‚úÖ Now You Can Run db-force-reinit.sh!

I've fixed the script so it will work correctly:

### What I Changed:

1. **Created** `/services/init-scripts/04-dataset-collections.sql`
   - Creates the `dataset_collections` table
   - Sets up indexes and triggers
   - Will be automatically run during database initialization

2. **Updated** `/scripts/db-reinit.sh`
   - Added `04-dataset-collections.sql` to the list of init scripts (line 181)
   - Now the table will be created every time you reinitialize

## üöÄ Step-by-Step Procedure

### Step 1: Run the Force Reinit Script

```bash
cd /home/mewtwo/Zykairotis/claude-context-core
./scripts/db-force-reinit.sh
```

**What this does:**
1. ‚úÖ Terminates all database connections
2. ‚úÖ Drops `claude_context` schema (all tables)
3. ‚úÖ Recreates schema with extensions (pgvector, uuid-ossp, etc.)
4. ‚úÖ Runs init scripts:
   - `01-init-pgvector.sql` - PostgreSQL vector setup
   - `02-init-schema.sql` - Creates projects, datasets, chunks tables
   - `03-github-jobs.sql` - Creates GitHub job queue tables
   - `04-dataset-collections.sql` - **Creates dataset_collections table!** ‚≠ê
5. ‚úÖ Runs migrations (web_provenance, mesh_tables)
6. ‚úÖ Deletes ALL Qdrant collections (clean slate)
7. ‚úÖ Recreates cognee_db database
8. ‚úÖ Cleans Neo4j graph
9. ‚úÖ Restarts API server (recreates pg-boss tables)

**You'll be prompted:**
```
‚ö†Ô∏è  This will DROP and recreate the PostgreSQL schema and delete all Qdrant collections.
Proceed? (type 'reinit' to continue):
```

Type `reinit` and press Enter.

### Step 2: Verify the Setup

```bash
# Check that dataset_collections table exists
psql postgresql://postgres:code-context-secure-password@localhost:5533/claude_context \
  -c "\d claude_context.dataset_collections"
```

**Expected output:**
```
                          Table "claude_context.dataset_collections"
        Column         |           Type           | Modifiers
-----------------------+--------------------------+-----------
 id                    | uuid                     | PRIMARY KEY
 dataset_id            | uuid                     | NOT NULL
 collection_name       | text                     | UNIQUE
 vector_db_type        | text                     | DEFAULT 'qdrant'
 dimension             | integer                  | DEFAULT 768
 is_hybrid             | boolean                  | DEFAULT true
 point_count           | bigint                   | DEFAULT 0
 ...
```

### Step 3: Re-index Each Dataset

**Important:** Index each dataset separately so they get unique collections.

#### 3.1 Index Main Codebase (if you have local code)

```javascript
claudeContext.index({
  project: 'Hypr-Voice',
  dataset: 'main',
  path: '/path/to/your/hypr-voice/codebase'
})
```

This will create:
- Collection: `project_hypr_voice_dataset_main`
- Entry in `dataset_collections` table linking dataset to collection
- Vectors with proper `datasetId` and `projectId` metadata

#### 3.2 Index Pydantic AI Docs

```javascript
claudeContext.crawl({
  project: 'Hypr-Voice',
  dataset: 'pydantic-ai-docs-v2',
  url: 'https://ai.pydantic.dev/sitemap.xml',
  maxPages: 10000
})
```

This will create:
- Collection: `project_hypr_voice_dataset_pydantic_ai_docs_v2`
- Proper mapping in `dataset_collections`

#### 3.3 Index GitHub Repositories

```javascript
// Perplexity-Claude repo
claudeContext.indexGitHub({
  project: 'Hypr-Voice',
  dataset: 'perplexity-claude',
  repo: 'Zykairotis/Perplexity-claude'
})

// Claude-Context-Core repo
claudeContext.indexGitHub({
  project: 'Hypr-Voice',
  dataset: 'claude-context-core',
  repo: 'Zykairotis/claude-context-core'
})
```

Each will get its own collection.

### Step 4: Verify Dataset Isolation

```javascript
// Check dataset list
claudeContext.listDatasets({ project: 'Hypr-Voice' })
```

**You should see:**
```
‚Ä¢ main:
  Status: active
  PostgreSQL: X,XXX chunks
  Qdrant: X,XXX vectors
  Collection: project_hypr_voice_dataset_main  ‚úÖ
  
‚Ä¢ pydantic-ai-docs-v2:
  Status: active
  PostgreSQL: X,XXX chunks
  Qdrant: X,XXX vectors
  Collection: project_hypr_voice_dataset_pydantic_ai_docs_v2  ‚úÖ
  
‚Ä¢ perplexity-claude:
  Status: active
  PostgreSQL: X,XXX chunks
  Qdrant: X,XXX vectors
  Collection: project_hypr_voice_dataset_perplexity_claude  ‚úÖ
```

**Notice:** Each dataset has its **OWN collection** now!

### Step 5: Test Dataset Filtering

```javascript
// Search ONLY pydantic-ai docs
claudeContext.search({
  project: 'Hypr-Voice',
  dataset: 'pydantic-ai-docs-v2',
  query: 'how to use PydanticAI'
})

// Should return ONLY results from pydantic-ai-docs-v2 ‚úÖ

// Search ONLY your local code
claudeContext.search({
  project: 'Hypr-Voice',
  dataset: 'main',
  query: 'TTS voice configuration'
})

// Should return ONLY results from main dataset ‚úÖ
```

## üéØ Why This Works Now

### Before (Broken):
```
PostgreSQL:
  ‚îú‚îÄ dataset: main
  ‚îú‚îÄ dataset: pydantic-ai-docs-v2
  ‚îú‚îÄ dataset: perplexity-claude
  ‚îî‚îÄ dataset: claude-context-core
       ‚Üì (no mappings)
       
Qdrant:
  ‚îî‚îÄ project_hypr_voice (ALL data mixed!) ‚ùå

Search: Always uses project_hypr_voice ‚Üí mixed results
```

### After (Fixed):
```
PostgreSQL:
  ‚îú‚îÄ dataset: main ‚Üí collection: project_hypr_voice_dataset_main
  ‚îú‚îÄ dataset: pydantic-ai-docs-v2 ‚Üí project_hypr_voice_dataset_pydantic_ai_docs_v2
  ‚îú‚îÄ dataset: perplexity-claude ‚Üí project_hypr_voice_dataset_perplexity_claude
  ‚îî‚îÄ dataset: claude-context-core ‚Üí project_hypr_voice_dataset_claude_context_core
       ‚Üì (mappings in dataset_collections table) ‚úÖ
       
Qdrant:
  ‚îú‚îÄ project_hypr_voice_dataset_main (only main data)
  ‚îú‚îÄ project_hypr_voice_dataset_pydantic_ai_docs_v2 (only docs)
  ‚îú‚îÄ project_hypr_voice_dataset_perplexity_claude (only this repo)
  ‚îî‚îÄ project_hypr_voice_dataset_claude_context_core (only this repo)

Search: Uses dataset_collections to find the right collection ‚Üí isolated results ‚úÖ
```

## üìä What the Filter Fix Does

The filter fix I made earlier in `/src/api/query.ts`:

```typescript
// Builds Qdrant filter expression
const qdrantFilter = buildQdrantFilter(filter);
// Example: "(metadata.datasetId == "uuid1" OR metadata.datasetId == "uuid2")"

// Passes filter to all search methods
context.dualModelSearch(..., qdrantFilter, ...)
vectorDb.search(..., { filterExpr: qdrantFilter })
```

**Combined with proper collection mapping:**
1. ‚úÖ Query resolves dataset name ‚Üí dataset ID
2. ‚úÖ Looks up collection name in `dataset_collections` table
3. ‚úÖ Searches ONLY that collection in Qdrant
4. ‚úÖ Additionally filters by `datasetId` metadata (belt + suspenders)
5. ‚úÖ Returns ONLY results from specified dataset

## üéâ Summary

**Now you can run:**
```bash
./scripts/db-force-reinit.sh
```

**Then re-index, and dataset isolation will work perfectly!**

The script now:
- ‚úÖ Creates `dataset_collections` table automatically
- ‚úÖ Sets up proper schema for collection tracking
- ‚úÖ Cleans everything for a fresh start
- ‚úÖ Works every time you reinitialize

# Reality Check: What's Actually Implemented vs The Plan

> **Date:** November 5, 2025
> 
> **Purpose:** Document actual implementation status vs plan assumptions

---

## üî¥ Critical Discrepancies

### 1. Collection Naming - PATH-BASED, NOT UUID-BASED ‚ùå

**Plan Assumes:**
```typescript
// UUID-based: proj_{PROJECT_ID}_{DATASET_ID}
generateCollectionName({ projectId, datasetId });
// Result: proj_a1b2c3d4_e5f6g7h8
```

**Reality:**
```typescript
// Path-based (MD5 hash)
// Location: /src/context.ts:294-300
public getCollectionName(codebasePath: string): string {
    const normalizedPath = path.resolve(codebasePath);
    const hash = crypto.createHash('md5').update(normalizedPath).digest('hex');
    const prefix = isHybrid ? 'hybrid_code_chunks' : 'code_chunks';
    return `${prefix}_${hash.substring(0, 8)}`;
}
// Result: hybrid_code_chunks_8c069df5
```

**Impact:** 
- ‚ùå Same path = same collection across different projects
- ‚ùå Breaks on project renames (path changes)
- ‚ùå No project isolation
- ‚ùå Collisions possible

---

### 2. dataset_collections Table DOES NOT EXIST ‚ùå

**Plan Assumes:**
- Table exists with columns: `collection_name`, `display_name`, `project_name`, `dataset_name`, etc.
- One collection per dataset tracked in database

**Reality:**
- ‚ùå Table does NOT exist in `/services/init-scripts/02-init-schema.sql`
- ‚ùå No migration script for this table
- ‚ùå No tracking of collection ‚Üí dataset relationship in database
- ‚ùå CollectionManager class does NOT exist

**Implications:**
- Can't track which collection belongs to which dataset
- Can't cache human-readable names
- Can't sync point counts
- Can't handle renames properly

---

### 3. indexed_files Table - Missing collection_name Column ‚ùå

**Plan Assumes:**
```sql
ALTER TABLE indexed_files
ADD COLUMN collection_name TEXT
  REFERENCES dataset_collections(collection_name);
```

**Reality:**
```sql
-- From /scripts/migrate-add-indexed-files.sh:81-96
CREATE TABLE IF NOT EXISTS claude_context.indexed_files (
    project_id UUID NOT NULL,
    dataset_id UUID NOT NULL,
    file_path TEXT NOT NULL,
    relative_path TEXT NOT NULL,
    sha256_hash TEXT NOT NULL,
    file_size BIGINT NOT NULL,
    chunk_count INTEGER DEFAULT 0,
    language TEXT,
    -- NO collection_name column!
    UNIQUE(project_id, dataset_id, file_path)
);
```

**Impact:**
- ‚ùå Incremental sync can't target specific collection
- ‚ùå Must search ALL collections to delete chunks for a file
- ‚ùå Performance penalty on modified files

---

### 4. deleteFileChunks Implementation - STUB ‚ùå

**Plan Assumes:**
- Targets specific collection
- Uses filters to delete chunks for a file

**Reality:**
```typescript
// From /src/sync/incremental-sync.ts:56-71
async function deleteFileChunks(
    context: Context,
    filePath: string,
    project: string,
    dataset: string
): Promise<number> {
    // STUB - doesn't actually delete!
    console.warn(`[IncrementalSync] Chunk deletion for ${filePath} not fully implemented yet`);
    return 0;
}
```

**Impact:**
- ‚ùå Modified files leave orphaned chunks in vector DB
- ‚ùå Wastes storage space
- ‚ùå Can return duplicate/stale results in queries

---

### 5. Query Logic - Searches ALL Collections ‚ùå

**Plan Assumes:**
- Project-scoped queries
- Only searches collections for specific project

**Reality:**
```typescript
// From /src/api/query.ts:390-394
const allCollections: string[] = await vectorDb.listCollections();
const hybridCollections = allCollections.filter(name => 
  name.startsWith('hybrid_code_chunks_') || name.startsWith('project_')
);
candidateCollections = hybridCollections;
// Searches EVERYTHING matching pattern!
```

**Impact:**
- ‚ùå Slow queries (searches all projects)
- ‚ùå No project isolation
- ‚ùå Cross-project contamination possible
- ‚ùå Scales poorly with multiple projects

---

### 6. indexWebPages - STUB IMPLEMENTATION ‚ùå

**Plan Assumes:**
- Full web page indexing with chunking
- SPLADE sparse vectors
- Project/dataset tracking

**Reality:**
```typescript
// From /src/context.ts:992-1001
async indexWebPages(pages, project, dataset, options?) {
    // STUB - not implemented!
    console.warn('[Context] indexWebPages not fully implemented');
    return { indexedPages: 0, totalChunks: 0 };
}
```

**Impact:**
- ‚ùå Web content ingestion doesn't work through Context
- ‚ö†Ô∏è Alternative path exists via `ingestCrawlPages` (Postgres-only)

---

### 7. Global System Project - DOES NOT EXIST ‚ùå

**Plan Assumes:**
```sql
INSERT INTO projects (id, name, is_system)
VALUES ('00000000-0000-0000-0000-000000000000', 'global', true);
```

**Reality:**
```sql
-- From /services/init-scripts/02-init-schema.sql:13-21
CREATE TABLE IF NOT EXISTS projects (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name TEXT NOT NULL UNIQUE,
  -- NO is_system column!
  is_global BOOLEAN NOT NULL DEFAULT false
);
```

**Impact:**
- ‚ùå No special system project for global datasets
- ‚ö†Ô∏è `is_global` flag exists on projects but not `is_system`

---

### 8. Datasets Table - Missing source_* Columns ‚ùå

**Plan Assumes:**
```sql
ALTER TABLE datasets
ADD COLUMN source_type TEXT CHECK (source_type IN ('github', 'local', 'crawl'));
ADD COLUMN source_metadata JSONB DEFAULT '{}'::jsonb;
```

**Reality:**
```sql
-- From /services/init-scripts/02-init-schema.sql:24-34
CREATE TABLE IF NOT EXISTS datasets (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  project_id UUID NOT NULL,
  name TEXT NOT NULL,
  status TEXT NOT NULL DEFAULT 'active',
  is_global BOOLEAN NOT NULL DEFAULT false,
  -- NO source_type column!
  -- NO source_metadata column!
);
```

---

## ‚úÖ What IS Implemented

### Crawl4AI Service (Python) - FULLY IMPLEMENTED ‚úÖ

**Location:** `/services/crawl4ai-runner/app/storage/scope_manager.py`

**Features:**
```python
class ScopeManager:
    def resolve_scope(project, dataset, requested_scope) -> ScopeLevel:
        # Returns: GLOBAL, PROJECT, or LOCAL
    
    def get_collection_name(project, dataset, scope) -> str:
        # Global: 'global_knowledge'
        # Project: 'project_{sanitized_name}'
        # Local: 'project_{sanitized_name}_dataset_{sanitized_name}'
    
    def get_project_id(project) -> str:
        # Deterministic UUID from name via uuid5
    
    def get_dataset_id(dataset) -> str:
        # Deterministic UUID from name via uuid5
    
    def filter_by_scope(scope, project_id, dataset_id) -> Dict:
        # Generate database filters for queries
```

**Integration:**
- ‚úÖ Used in `crawling_service.py` for web page ingestion
- ‚úÖ Creates collections: `project_{name}_dataset_{name}`
- ‚úÖ Stores to both Postgres (canonical) and Qdrant (vectors)
- ‚úÖ CanonicalMetadataStore syncs projects, datasets, web_pages, chunks

**Example from crawling_service.py:980-1056:**
```python
scope_manager = ScopeManager()
scope = scope_manager.resolve_scope(ctx.project, ctx.dataset, ctx.scope)
collection_name = scope_manager.get_collection_name(
    ctx.project, ctx.dataset, scope
)
# Result: "project_myapp_dataset_frontend"
```

**Architecture Choice:**
- Uses **NAME-based** collections, not UUID-based
- Sanitizes names (lowercase, alphanumeric + underscores)
- Simple and debuggable
- BUT: Different from plan's UUID approach

---

### Database Schema - PARTIAL ‚úÖ

**Core tables exist:**
- ‚úÖ `projects` - with is_global flag
- ‚úÖ `datasets` - with is_global flag
- ‚úÖ `documents` - with dataset_id FK
- ‚úÖ `web_pages` - with dataset_id FK
- ‚úÖ `chunks` - with dataset_id, document_id, web_page_id FKs
- ‚úÖ `crawl_sessions` - with dataset_id FK
- ‚úÖ `project_shares` - for cross-project resource sharing
- ‚úÖ `indexed_files` - for incremental sync (see below)

**Missing columns:**
- ‚ùå `projects.is_system` - only has is_global
- ‚ùå `datasets.source_type` - no column
- ‚ùå `datasets.source_metadata` - no column
- ‚ùå `indexed_files.collection_name` - no column

**Missing tables:**
- ‚ùå `dataset_collections` - entire table doesn't exist

---

### indexed_files Table - PARTIAL ‚úÖ

**Location:** Created by migration script, used by incremental sync

**Current schema:**
```sql
CREATE TABLE indexed_files (
  id UUID PRIMARY KEY,
  project_id UUID NOT NULL,
  dataset_id UUID NOT NULL,
  file_path TEXT NOT NULL,
  relative_path TEXT NOT NULL,
  sha256_hash TEXT NOT NULL,
  file_size BIGINT NOT NULL,
  chunk_count INTEGER DEFAULT 0,
  language TEXT,
  last_indexed_at TIMESTAMPTZ,
  metadata JSONB,
  created_at TIMESTAMPTZ,
  updated_at TIMESTAMPTZ,
  UNIQUE(project_id, dataset_id, file_path)
);
```

**What's used:**
- ‚úÖ Change detection (compare sha256_hash)
- ‚úÖ Rename detection (find matching hashes)
- ‚úÖ File metadata tracking
- ‚úÖ Incremental sync pipeline

**What's missing:**
- ‚ùå `collection_name` column (can't target specific collection for chunk deletion)
- ‚ùå FK to `dataset_collections` (table doesn't exist)

---

## ‚ùå What's NOT Implemented

### TypeScript Context Class - LEGACY ARCHITECTURE üî¥

### 1. Incremental Sync Core ‚úÖ

**Files:**
- `/src/sync/change-detector.ts` - Full SHA256 change detection
- `/src/sync/file-metadata.ts` - CRUD operations for file metadata
- `/src/sync/hash-calculator.ts` - SHA256 utilities
- `/src/sync/incremental-sync.ts` - Main sync orchestration (with stub delete)

**Status:** ~80% complete
- ‚úÖ Change detection works
- ‚úÖ File metadata tracking works
- ‚úÖ Rename detection works
- ‚ùå Chunk deletion is stub
- ‚ùå No collection_name tracking

### 2. File Watcher ‚úÖ

**Files:**
- `/src/sync/file-watcher.ts` (implied by API routes)
- API endpoints in `/services/api-server/src/routes/projects.ts`

**Status:** Likely working but untested

### 3. Project/Dataset Schema ‚úÖ

**Tables:**
- ‚úÖ `projects` table exists
- ‚úÖ `datasets` table exists
- ‚úÖ `project_shares` table exists
- ‚úÖ Foreign key relationships correct

### 4. Hybrid Search ‚úÖ

**Implementation:**
- ‚úÖ SPLADE integration working
- ‚úÖ Dense + sparse search
- ‚úÖ Reranking support
- ‚úÖ Fallback logic

### 5. Web Query (Separate Path) ‚úÖ

**Implementation:**
- ‚úÖ `queryWebContent` function exists
- ‚úÖ Uses project/dataset filtering
- ‚úÖ Hybrid search support
- ‚ö†Ô∏è Uses different collection naming (`web_${project}`)

---

## üìä Implementation Status Summary

| Component | Plan Status | Reality Status | Gap |
|-----------|-------------|----------------|-----|
| **Collection Naming** | UUID-based immutable | Path-based MD5 hash | üî¥ Critical |
| **dataset_collections Table** | Exists | Does NOT exist | üî¥ Critical |
| **indexed_files.collection_name** | Exists | Does NOT exist | üî¥ Critical |
| **CollectionManager Class** | Implemented | Does NOT exist | üî¥ Critical |
| **deleteFileChunks** | Full implementation | Stub (returns 0) | üî¥ Critical |
| **Query Project Scoping** | Project-scoped | Searches ALL | üî¥ Critical |
| **indexWebPages** | Full implementation | Stub | üî¥ Critical |
| **is_system Column** | Exists | Does NOT exist | üü° Medium |
| **source_type/metadata** | Exists | Does NOT exist | üü° Medium |
| **Change Detection** | Implemented | ‚úÖ Implemented | ‚úÖ Good |
| **File Metadata CRUD** | Implemented | ‚úÖ Implemented | ‚úÖ Good |
| **Hash Calculation** | Implemented | ‚úÖ Implemented | ‚úÖ Good |
| **Hybrid Search** | Implemented | ‚úÖ Implemented | ‚úÖ Good |

---

## üö® Required Plan Updates

### Priority 1: Database Schema (Must Have First)

1. **Create dataset_collections table**
   - Collection tracking
   - Point count sync
   - Display names
   
2. **Add collection_name to indexed_files**
   - Link files to collections
   - Enable targeted deletion

3. **Add is_system to projects**
   - Support global system project

4. **Add source_type/metadata to datasets**
   - Track content origin

### Priority 2: Core Utilities (Foundation)

1. **Implement UUID-based collection naming**
   - Replace path-based hashing
   - Use project_id + dataset_id
   
2. **Create CollectionManager class**
   - CRUD for collections
   - Error recovery
   - Point count sync

3. **Fix deleteFileChunks**
   - Implement actual deletion
   - Target specific collection
   - Use collection_name from indexed_files

### Priority 3: Query & Indexing (Features)

1. **Update query logic**
   - Project-scoped collection search
   - Use dataset_collections table
   
2. **Implement indexWebPages**
   - Full web content indexing
   - Use CollectionManager

3. **Update incremental sync**
   - Pass collection_name
   - Use new deleteFileChunks

---

## üéØ Corrected Implementation Order

### Phase 0: Schema Migrations (Week 1, Days 1-2)
```bash
# Run these in order
./scripts/migrate-add-indexed-files.sh           # ‚úÖ Already exists
./scripts/migrate-add-dataset-collections.sh     # ‚ùå Must create
./scripts/migrate-add-collection-column.sh       # ‚ùå Must create  
./scripts/migrate-add-source-columns.sh          # ‚ùå Must create
./scripts/migrate-add-is-system-column.sh        # ‚ùå Must create
```

### Phase 1: Fix Collection Naming (Week 1, Days 3-4)
- Replace `getCollectionName()` with UUID-based version
- Create `collection-names.ts` utilities
- Update all references

### Phase 2: Create CollectionManager (Week 1, Day 5)
- Implement class with error recovery
- Add to Context initialization

### Phase 3: Fix Incremental Sync (Week 2, Days 1-2)
- Implement real deleteFileChunks
- Add collection_name tracking
- Update sync workflow

### Phase 4: Fix Query Logic (Week 2, Days 3-4)
- Project-scoped collection selection
- Remove redundant filters

### Phase 5: Implement indexWebPages (Week 2, Day 5)
- Full implementation
- Use CollectionManager

---

## ‚úÖ Updated Checklist Reference

See updated implementation checklist that reflects reality:
- Migration scripts to create
- Actual implementation gaps
- Correct dependency order

---

**Status:** üî¥ **MAJOR GAPS IDENTIFIED - PLAN NEEDS SIGNIFICANT UPDATES**

**Recommendation:** 
1. ‚úÖ Run existing migrations first
2. Create missing migrations
3. Fix core utilities (collection naming, manager)
4. Then proceed with features
